{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章 逻辑斯谛回归与最大熵模型\n",
    "## 6.1 预备知识（逻辑斯谛回归）\n",
    "逻辑斯谛回归模型<b>假定随机变量X的数据分布符合逻辑斯谛回归</b>,有:  \n",
    ">逻辑斯谛回归分布函数：\n",
    "$$F(x)=P(X \\leq x)=\\frac{1}{1+e^\\left(-\\frac{x-\\mu}{\\gamma}\\right)}$$\n",
    "\n",
    ">密度函数：\n",
    "$$f(x)=F'(x)=\\frac{e^\\left(-\\frac{x-\\mu}{\\gamma}\\right)}{\\gamma(1+e^\\left(-\\frac{x-\\mu}{\\gamma}\\right))^2}$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 二项逻辑斯谛回归模型\n",
    "抛开上述关于逻辑斯谛回归的预备知识，这里给出二项逻辑斯谛回归模型的<b>定义</b>：\n",
    "><b>给定模型参数$w$和一个样本$x$，逻辑斯谛逻辑回归模型</b>\n",
    "$$P(Y=1|x) = \\frac{e^\\left(wx+b\\right)}{1+e^\\left(wx+b\\right)}$$  \n",
    "$$P(Y=0|x) = \\frac{1}{1+e^\\left(wx+b\\right)}$$\n",
    "\n",
    "其中，$Y \\in \\{0,1\\}$为二分类的类别标签空间，$x$是数据的一条样本记录，通过上述的两个式子得到的结果就是在给定了$x$的情况下，分别得到类别标签为$0$和$1$的概率值，所以，有心的读者可以发现，这个思维和朴素贝叶斯的思路很像，都是通过给定样本$x$得到不同类别标签的概率值，并选择类别概率值最大的类别作为预测类别。那~ 我们为了得到参数$w$和$b$（实际可以理解为只需要求解$w$，如下式），需要做哪些工作呢？\n",
    ">$$P(Y=1|x) = \\frac{e^\\left(wx\\right)}{1+e^\\left(wx\\right)}$$  \n",
    "$$P(Y=0|x) = \\frac{1}{1+e^\\left(wx\\right)}$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 求解逻辑斯谛回归的参数$w$\n",
    "### 6.3.1 转化问题为似然函数求解问题\n",
    "根据6.2中的式子可以得知\n",
    "$$P(Y=1|x) = \\frac{e^\\left(wx\\right)}{1+e^\\left(wx\\right)}$$\n",
    "$$ = 1 - P(Y=0|x)$$\n",
    "\n",
    "令\n",
    "$$P(Y=1|x) = \\pi(x)$$\n",
    "\n",
    "则有\n",
    "$$P(Y=0|x) = 1- \\pi(x)$$\n",
    "\n",
    "(其中，$\\pi(x)$是由参数$w$决定的函数)\n",
    "$$\\pi(x) = \\frac{e^\\left(wx\\right)}{1+e^\\left(wx\\right)} $$\n",
    ">(1)此时可以直接使用似然函数求解得到$w$的值，其中，作为补充，这里给出似然函数的表达，即有：\n",
    "$$L(w) = \\prod_{i=1}^N[\\pi(x_i)]^{y_i} [1-\\pi(x_i)]^{1-y_i}$$\n",
    "(2)写出对数似然函数\n",
    "$$\\log(L(w)) = \\sum_{i=1}^N[y_i \\log \\pi(x_i) + (1-y_i) \\log(1-\\pi(x_i))]$$\n",
    "(3)此时我们的目的，是求解<b>最大化</b>$L(w)$时的$w$值,即\n",
    "$$w^* = \\arg\\underset{w}{\\max} (\\log(L(w))$$\n",
    "\n",
    "那，如何求解最大化对数函数的参数值呢？处理这种高维度参数的问题，实际上都是使用凸优化的通用的通法（梯度下降或者拟牛顿法来求解极值）了，所以这里使用梯度下降方法来求解参数$w$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 求解似然函数极值\n",
    "使用<b>梯度下降方法</b>（这里是梯度上升）求解极（大）值\n",
    ">结合之前在感知机模型中关于梯度下降方法的描述，这里首先对$L(w)$求解关于$w$的偏导数，即有\n",
    "$$\\nabla_w L(w)\n",
    "= \\nabla_w (\\sum_{i=1}^N[y_i \\log \\pi(x_i) + (1-y_i) \\log(1-\\pi(x_i))])$$\n",
    "$$= \\sum_{i=1}^N(\\frac{y_i}{\\pi(x_i)} - (1-y_i) \\frac{1}{1-\\pi(x_i)}) \\nabla_w (\\pi(x_i))$$\n",
    "其中\n",
    "$$\\nabla_w (\\pi(x_i))= \\frac{(1 + e^{w x_i}) e^{w x_i} x_i - e^{wx_i}e^{wx_i}x_i}{(1+e^{wx})^2} = x_i \\pi(x_i) (1-\\pi(x_i))$$\n",
    "上式变为\n",
    "$$= \\sum_{i=1}^N (\\frac{y_i}{\\pi(x_i)} - (1-y_i) \\frac{1}{1-\\pi(x_i)}) \\pi(x_i) (1-\\pi(x_i)) x_i$$\n",
    "化简为\n",
    "$$= \\sum_{i=1}^N y_i (1-\\pi(x_i)) x_i - (1-y_i)\\pi(x_i)x_i$$\n",
    "$$= \\sum_{i=1}^N (y_i- \\pi(x_i))x_i$$\n",
    "当给定了步长$\\eta$的值时，所以每次更新参数$w$的操作为：\n",
    ">$$\\Delta w \\leftarrow \\eta \\sum_{i=1}^N (y_i- \\pi(x_i))x_i$$\n",
    "也即有\n",
    "$$w_{t+1} = w_{t} + \\Delta w$$\n",
    "$$ = w_{t} + \\eta \\sum_{i=1}^N (y_i- \\frac{e^\\left(w_t x_i\\right)}{1+e^\\left(w_t x_i\\right)})x_i$$\n",
    "至此，目标函数已经得到，为了求解逻辑斯谛回归的参数$w^*$。接下来将对以上的思路进行求解并编写代码实现：\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 如何实现\n",
    "### 6.4.1 自定义数据集\n",
    ">首先定义数据$X$和$Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[3,4],[1,1],[4,3],[1,0],[2,1],[4,2],[4,0]]) # 定义三个数据向量(8行_2列,数组)\n",
    "y = np.array([1,0,1,0,0,1,1]).T # 定义类别标签(8行_1列,向量)\n",
    "\n",
    "posindex = x[y[:] == 1] # 绘图用，标记正样本id\n",
    "negindex = x[y[:] == 0] # 绘图用，标记负样本id\n",
    "\n",
    "# 绘制散点图\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(0)\n",
    "plt.scatter(posindex[:,0], posindex[:,1], c='red', alpha=1, marker='+', label='pickup') \n",
    "plt.scatter(negindex[:,0], negindex[:,1], c='green', alpha=1, marker='o', label='pickup') \n",
    "plt.axis([0,5,-1,5])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">此时我们的目标是求解参数$w$  \n",
    "(注意，这里由于需要得到$wx$的值，即$w$和$x$应该可以相乘，所以，这就隐式地要求$w$的参数为两个数，需要定义为一个包含两个参数值得参数向量：$w = (w_1,w_2)$)。但是这里我们这样设置也不合理，因为在上面我们已经将$wx+b$变相处理为了$wx$，所以这里统一对输入的X添加一个纵列，即$np.ones(len(X))$，即$wx = w_0x_0+w_1x_1+w_2x_2$，因为$x_0$都是1，所以可以理解为$wx = w_0+w_1x_1+w_2x_2$，这样$w_0$就相当于参数$b$了。\n",
    "\n",
    "### 6.4.2 编写损失计算函数、梯度上升求解w的函数、预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算损失\n",
    "def calLoss(w,X,Y):\n",
    "    return np.sum((np.exp(np.dot(w,X.T))/(np.exp(np.dot(w,X.T)) + 1) - Y)**2)/2\n",
    "\n",
    "# 预测，带入式子π(x)，得到y_，若y_大于0.5，则预测y为1，否则为0\n",
    "def predict(w,X):\n",
    "    result = []\n",
    "    X = np.column_stack([np.ones(len(X)),X])\n",
    "    y_ = np.exp(np.dot(w,X.T))/(np.exp(np.dot(w,X.T)) + 1.0)\n",
    "    y = y_\n",
    "    y[y>0.5] = 1  \n",
    "    y[y<=0.5] = 0\n",
    "    return y\n",
    "\n",
    "# 梯度上升算法求解最大化似然函数的时候的参数w\n",
    "def gradAscent(X,Y,eta,iter):\n",
    "    row, col = X.shape\n",
    "    X = np.column_stack([np.ones(len(x)),X])\n",
    "    w = np.ones((1,col + 1))\n",
    "    for i in range(iter):\n",
    "        ## 以下三步更新参数w\n",
    "        grad = np.dot((Y - np.exp(np.dot(w,X.T))/(1 + np.exp(np.dot(w,X.T)))),X) # step 1 \n",
    "        delta = eta * grad # step 2\n",
    "        w = w + delta # step 3\n",
    "        if i%10 == 0:\n",
    "            print('==============================iteration'+ str(i)+'==============================')\n",
    "            print 'weight:\\t\\t',w\n",
    "            print 'Loss:\\t\\t', calLoss(w,X,Y)\n",
    "            print 'pred labels:\\t',predict(w,x)\n",
    "        \n",
    "        if sum(abs(predict(w,x)[0].flatten() - y)) == 0:\n",
    "            print('=============================end at iter:'+ str(i)+'=============================')\n",
    "            print 'weight:\\t\\t',w\n",
    "            print 'Loss:\\t\\t', calLoss(w,X,Y)\n",
    "            print 'pred labels:\\t',predict(w,x)\n",
    "            break # 如果满足所有的类别标签已经预测正确，则可以直接早停并返回参数w\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================iteration0==============================\n",
      "weight:\t\t[[0.83157338 0.77412186 0.8841749 ]]\n",
      "Loss:\t\t1.2371932361860227\n",
      "pred labels:\t[[1. 1. 1. 1. 1. 1. 1.]]\n",
      "==============================iteration10==============================\n",
      "weight:\t\t[[-0.14660199  0.21201419  0.410074  ]]\n",
      "Loss:\t\t0.6294973683904217\n",
      "pred labels:\t[[1. 1. 1. 1. 1. 1. 1.]]\n",
      "==============================iteration20==============================\n",
      "weight:\t\t[[-0.7379793   0.42948221  0.36742371]]\n",
      "Loss:\t\t0.4760452571202963\n",
      "pred labels:\t[[1. 1. 1. 0. 1. 1. 1.]]\n",
      "==============================iteration30==============================\n",
      "weight:\t\t[[-1.24504627  0.60460239  0.36114593]]\n",
      "Loss:\t\t0.3687044061969869\n",
      "pred labels:\t[[1. 0. 1. 0. 1. 1. 1.]]\n",
      "==============================iteration40==============================\n",
      "weight:\t\t[[-1.68210559  0.75101819  0.3684553 ]]\n",
      "Loss:\t\t0.29381420364059524\n",
      "pred labels:\t[[1. 0. 1. 0. 1. 1. 1.]]\n",
      "==============================iteration50==============================\n",
      "weight:\t\t[[-2.06269681  0.87653135  0.38074443]]\n",
      "Loss:\t\t0.24079620825920617\n",
      "pred labels:\t[[1. 0. 1. 0. 1. 1. 1.]]\n",
      "=============================end at iter:57=============================\n",
      "weight:\t\t[[-2.30160758  0.95467156  0.39042785]]\n",
      "Loss:\t\t0.21266474848251013\n",
      "pred labels:\t[[1. 0. 1. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "w = gradAscent(x,y,0.06,200)\n",
    "r = predict(w,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
